{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = pd.read_csv('../data/processed/processed_listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>style</th>\n",
       "      <th>living_area</th>\n",
       "      <th>lot_dimensions</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>levels</th>\n",
       "      <th>location</th>\n",
       "      <th>listing_date</th>\n",
       "      <th>year_of_construction</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 storey</td>\n",
       "      <td>1191</td>\n",
       "      <td>4076</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Beauport</td>\n",
       "      <td>2020-12-01</td>\n",
       "      <td>2004</td>\n",
       "      <td>332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>1645</td>\n",
       "      <td>1360</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Mercier</td>\n",
       "      <td>2021-11-01</td>\n",
       "      <td>2006</td>\n",
       "      <td>612000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Link</td>\n",
       "      <td>2024</td>\n",
       "      <td>17000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Stoneham</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 storey</td>\n",
       "      <td>2400</td>\n",
       "      <td>4471</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Gatineau</td>\n",
       "      <td>2021-12-01</td>\n",
       "      <td>1989</td>\n",
       "      <td>360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Unknown</td>\n",
       "      <td>1800</td>\n",
       "      <td>16090</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Alma</td>\n",
       "      <td>2021-09-01</td>\n",
       "      <td>4808</td>\n",
       "      <td>284000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      style  living_area  lot_dimensions  bedrooms  bathrooms  levels  \\\n",
       "0  2 storey         1191            4076         3          1       2   \n",
       "1   Unknown         1645            1360         3          1       3   \n",
       "2      Link         2024           17000         4          3       1   \n",
       "3  2 storey         2400            4471         4          2       2   \n",
       "4   Unknown         1800           16090         5          2       2   \n",
       "\n",
       "   location listing_date  year_of_construction   price  \n",
       "0  Beauport   2020-12-01                  2004  332500  \n",
       "1   Mercier   2021-11-01                  2006  612000  \n",
       "2  Stoneham   2021-12-01                  2019  526500  \n",
       "3  Gatineau   2021-12-01                  1989  360000  \n",
       "4      Alma   2021-09-01                  4808  284000  "
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.insert(7, 'year', '')\n",
    "listings_df['year'] = pd.DatetimeIndex(listings_df['listing_date']).year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "def plot_corr_map(df):\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_corr_map(listings_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = ['location', 'year']\n",
    "one_hot_cols = ['bedrooms', 'bathrooms']\n",
    "numerical_cols = ['living_area', 'lot_dimensions']\n",
    "target_col = ['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df = listings_df[ordinal_cols + one_hot_cols + numerical_cols + target_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oh_encode(df_line, col, new_col, val):\n",
    "    if df_line[col] == val:\n",
    "        return df_line[new_col] + 1\n",
    "    else:\n",
    "        return df_line[new_col]\n",
    "\n",
    "for col in one_hot_cols:\n",
    "    for val in listings_df[col].unique():   \n",
    "        new_col = str(val) + '_' + col\n",
    "        listings_df[new_col] = 0\n",
    "        listings_df[new_col] = listings_df.apply(oh_encode, args=(col, new_col, val), axis=1)\n",
    "\n",
    "    listings_df = listings_df.drop(columns=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "      <th>living_area</th>\n",
       "      <th>lot_dimensions</th>\n",
       "      <th>price</th>\n",
       "      <th>3_bedrooms</th>\n",
       "      <th>4_bedrooms</th>\n",
       "      <th>5_bedrooms</th>\n",
       "      <th>6_bedrooms</th>\n",
       "      <th>2_bedrooms</th>\n",
       "      <th>1_bedrooms</th>\n",
       "      <th>1_bathrooms</th>\n",
       "      <th>3_bathrooms</th>\n",
       "      <th>2_bathrooms</th>\n",
       "      <th>4_bathrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beauport</td>\n",
       "      <td>2020</td>\n",
       "      <td>1191</td>\n",
       "      <td>4076</td>\n",
       "      <td>332500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mercier</td>\n",
       "      <td>2021</td>\n",
       "      <td>1645</td>\n",
       "      <td>1360</td>\n",
       "      <td>612000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Stoneham</td>\n",
       "      <td>2021</td>\n",
       "      <td>2024</td>\n",
       "      <td>17000</td>\n",
       "      <td>526500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gatineau</td>\n",
       "      <td>2021</td>\n",
       "      <td>2400</td>\n",
       "      <td>4471</td>\n",
       "      <td>360000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alma</td>\n",
       "      <td>2021</td>\n",
       "      <td>1800</td>\n",
       "      <td>16090</td>\n",
       "      <td>284000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location  year  living_area  lot_dimensions   price  3_bedrooms  \\\n",
       "0  Beauport  2020         1191            4076  332500           1   \n",
       "1   Mercier  2021         1645            1360  612000           1   \n",
       "2  Stoneham  2021         2024           17000  526500           0   \n",
       "3  Gatineau  2021         2400            4471  360000           0   \n",
       "4      Alma  2021         1800           16090  284000           0   \n",
       "\n",
       "   4_bedrooms  5_bedrooms  6_bedrooms  2_bedrooms  1_bedrooms  1_bathrooms  \\\n",
       "0           0           0           0           0           0            1   \n",
       "1           0           0           0           0           0            1   \n",
       "2           1           0           0           0           0            0   \n",
       "3           1           0           0           0           0            0   \n",
       "4           0           1           0           0           0            0   \n",
       "\n",
       "   3_bathrooms  2_bathrooms  4_bathrooms  \n",
       "0            0            0            0  \n",
       "1            0            0            0  \n",
       "2            1            0            0  \n",
       "3            0            1            0  \n",
       "4            0            1            0  "
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = listings_df[target_col]\n",
    "X = listings_df.drop(columns=target_col)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoders = {}\n",
    "\n",
    "for col in ordinal_cols:\n",
    "    ordinal_encoders[col] = OrdinalEncoder()\n",
    "    X_train[col] = ordinal_encoders[col].fit_transform(X_train[[col]])\n",
    "    X_test[col] = ordinal_encoders[col].transform(X_test[[col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    scalers[col] = MinMaxScaler()\n",
    "    X_train[col] = scalers[col].fit_transform(X_train[[col]])\n",
    "    X_test[col] = scalers[col].transform(X_test[[col]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_target = True\n",
    "\n",
    "if scale_target:\n",
    "    target_scaler = MinMaxScaler()\n",
    "\n",
    "    y_train = target_scaler.fit_transform(y_train)\n",
    "    y_test = target_scaler.fit_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "      <th>living_area</th>\n",
       "      <th>lot_dimensions</th>\n",
       "      <th>3_bedrooms</th>\n",
       "      <th>4_bedrooms</th>\n",
       "      <th>5_bedrooms</th>\n",
       "      <th>6_bedrooms</th>\n",
       "      <th>2_bedrooms</th>\n",
       "      <th>1_bedrooms</th>\n",
       "      <th>1_bathrooms</th>\n",
       "      <th>3_bathrooms</th>\n",
       "      <th>2_bathrooms</th>\n",
       "      <th>4_bathrooms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33706</th>\n",
       "      <td>64.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.190462</td>\n",
       "      <td>0.094329</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889</th>\n",
       "      <td>46.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.158671</td>\n",
       "      <td>0.099474</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24390</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.321676</td>\n",
       "      <td>0.095902</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22221</th>\n",
       "      <td>59.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.194798</td>\n",
       "      <td>0.061926</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14741</th>\n",
       "      <td>37.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.306069</td>\n",
       "      <td>0.047838</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       location  year  living_area  lot_dimensions  3_bedrooms  4_bedrooms  \\\n",
       "33706      64.0   5.0     0.190462        0.094329           1           0   \n",
       "1889       46.0  13.0     0.158671        0.099474           1           0   \n",
       "24390      17.0   8.0     0.321676        0.095902           0           1   \n",
       "22221      59.0   9.0     0.194798        0.061926           0           0   \n",
       "14741      37.0  11.0     0.306069        0.047838           0           1   \n",
       "\n",
       "       5_bedrooms  6_bedrooms  2_bedrooms  1_bedrooms  1_bathrooms  \\\n",
       "33706           0           0           0           0            0   \n",
       "1889            0           0           0           0            0   \n",
       "24390           0           0           0           0            0   \n",
       "22221           0           0           1           0            1   \n",
       "14741           0           0           0           0            1   \n",
       "\n",
       "       3_bathrooms  2_bathrooms  4_bathrooms  \n",
       "33706            0            1            0  \n",
       "1889             0            1            0  \n",
       "24390            0            1            0  \n",
       "22221            0            0            0  \n",
       "14741            0            0            0  "
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37468226],\n",
       "       [0.34722928],\n",
       "       [0.3024911 ],\n",
       "       ...,\n",
       "       [0.35332994],\n",
       "       [0.23335028],\n",
       "       [0.20691408]])"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28718737446340814"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression().fit(X_train, y_train)\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_36 (Dense)            (None, 16)                240       \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 385\n",
      "Trainable params: 385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\simon\\Documents\\GitHub\\quebec-real-estate\\venv\\lib\\site-packages\\keras\\optimizer_v2\\adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 3ms/step - loss: 0.7195 - val_loss: 0.7072\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.7025 - val_loss: 0.6917\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6862 - val_loss: 0.6768\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6707 - val_loss: 0.6626\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6558 - val_loss: 0.6491\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6416 - val_loss: 0.6362\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6280 - val_loss: 0.6239\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6151 - val_loss: 0.6122\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.6011\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5909 - val_loss: 0.5904\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5803\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5690 - val_loss: 0.5707\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5587 - val_loss: 0.5615\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5489 - val_loss: 0.5527\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5396 - val_loss: 0.5443\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5306 - val_loss: 0.5363\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5221 - val_loss: 0.5286\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5140 - val_loss: 0.5213\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.5061 - val_loss: 0.5143\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4986 - val_loss: 0.5075\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4914 - val_loss: 0.5011\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4845 - val_loss: 0.4948\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4779 - val_loss: 0.4888\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4715 - val_loss: 0.4830\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4653 - val_loss: 0.4774\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4594 - val_loss: 0.4720\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4536 - val_loss: 0.4667\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4480 - val_loss: 0.4616\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4426 - val_loss: 0.4566\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4373 - val_loss: 0.4518\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4322 - val_loss: 0.4470\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4272 - val_loss: 0.4423\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4223 - val_loss: 0.4378\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4175 - val_loss: 0.4333\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4128 - val_loss: 0.4289\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4083 - val_loss: 0.4246\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4038 - val_loss: 0.4203\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3993 - val_loss: 0.4161\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3950 - val_loss: 0.4120\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3907 - val_loss: 0.4078\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3865 - val_loss: 0.4038\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3823 - val_loss: 0.3998\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3782 - val_loss: 0.3958\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3742 - val_loss: 0.3918\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3702 - val_loss: 0.3880\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3662 - val_loss: 0.3841\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3623 - val_loss: 0.3803\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3584 - val_loss: 0.3764\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3546 - val_loss: 0.3726\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 0.3689\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3470 - val_loss: 0.3651\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3433 - val_loss: 0.3614\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3396 - val_loss: 0.3578\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3359 - val_loss: 0.3541\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3323 - val_loss: 0.3505\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3287 - val_loss: 0.3469\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3251 - val_loss: 0.3433\n",
      "Epoch 58/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3215 - val_loss: 0.3397\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3180 - val_loss: 0.3361\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3144 - val_loss: 0.3326\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3109 - val_loss: 0.3291\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3075 - val_loss: 0.3256\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3040 - val_loss: 0.3221\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3006 - val_loss: 0.3186\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2971 - val_loss: 0.3152\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2937 - val_loss: 0.3117\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2904 - val_loss: 0.3083\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2870 - val_loss: 0.3049\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2837 - val_loss: 0.3016\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2804 - val_loss: 0.2982\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2771 - val_loss: 0.2948\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2738 - val_loss: 0.2915\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2706 - val_loss: 0.2882\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2673 - val_loss: 0.2849\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2641 - val_loss: 0.2817\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2609 - val_loss: 0.2784\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2577 - val_loss: 0.2752\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2546 - val_loss: 0.2720\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2515 - val_loss: 0.2688\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2484 - val_loss: 0.2656\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2453 - val_loss: 0.2625\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2422 - val_loss: 0.2593\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2392 - val_loss: 0.2562\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2361 - val_loss: 0.2531\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2331 - val_loss: 0.2501\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.2301 - val_loss: 0.2470\n",
      "Epoch 87/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2272 - val_loss: 0.2440\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2242 - val_loss: 0.2410\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2213 - val_loss: 0.2380\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2184 - val_loss: 0.2350\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2155 - val_loss: 0.2320\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2127 - val_loss: 0.2291\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2098 - val_loss: 0.2262\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2070 - val_loss: 0.2233\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2042 - val_loss: 0.2205\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2015 - val_loss: 0.2176\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1987 - val_loss: 0.2148\n",
      "Epoch 98/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1960 - val_loss: 0.2120\n",
      "Epoch 99/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1933 - val_loss: 0.2092\n",
      "Epoch 100/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1906 - val_loss: 0.2065\n",
      "Epoch 101/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1880 - val_loss: 0.2038\n",
      "Epoch 102/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1854 - val_loss: 0.2011\n",
      "Epoch 103/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1828 - val_loss: 0.1984\n",
      "Epoch 104/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1802 - val_loss: 0.1958\n",
      "Epoch 105/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1776 - val_loss: 0.1931\n",
      "Epoch 106/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1751 - val_loss: 0.1905\n",
      "Epoch 107/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1726 - val_loss: 0.1879\n",
      "Epoch 108/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1701 - val_loss: 0.1854\n",
      "Epoch 109/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1677 - val_loss: 0.1829\n",
      "Epoch 110/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1652 - val_loss: 0.1803\n",
      "Epoch 111/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1628 - val_loss: 0.1778\n",
      "Epoch 112/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1604 - val_loss: 0.1754\n",
      "Epoch 113/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1581 - val_loss: 0.1729\n",
      "Epoch 114/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1557 - val_loss: 0.1705\n",
      "Epoch 115/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1534 - val_loss: 0.1681\n",
      "Epoch 116/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1511 - val_loss: 0.1657\n",
      "Epoch 117/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1488 - val_loss: 0.1634\n",
      "Epoch 118/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1466 - val_loss: 0.1610\n",
      "Epoch 119/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1443 - val_loss: 0.1587\n",
      "Epoch 120/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1421 - val_loss: 0.1564\n",
      "Epoch 121/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1399 - val_loss: 0.1541\n",
      "Epoch 122/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1377 - val_loss: 0.1519\n",
      "Epoch 123/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1356 - val_loss: 0.1497\n",
      "Epoch 124/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1335 - val_loss: 0.1475\n",
      "Epoch 125/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1314 - val_loss: 0.1453\n",
      "Epoch 126/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1293 - val_loss: 0.1431\n",
      "Epoch 127/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1272 - val_loss: 0.1410\n",
      "Epoch 128/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1252 - val_loss: 0.1389\n",
      "Epoch 129/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1232 - val_loss: 0.1368\n",
      "Epoch 130/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1212 - val_loss: 0.1347\n",
      "Epoch 131/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1192 - val_loss: 0.1327\n",
      "Epoch 132/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1173 - val_loss: 0.1306\n",
      "Epoch 133/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1154 - val_loss: 0.1286\n",
      "Epoch 134/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1135 - val_loss: 0.1266\n",
      "Epoch 135/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1116 - val_loss: 0.1247\n",
      "Epoch 136/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1098 - val_loss: 0.1227\n",
      "Epoch 137/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1079 - val_loss: 0.1208\n",
      "Epoch 138/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1061 - val_loss: 0.1189\n",
      "Epoch 139/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1043 - val_loss: 0.1170\n",
      "Epoch 140/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1026 - val_loss: 0.1152\n",
      "Epoch 141/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1009 - val_loss: 0.1134\n",
      "Epoch 142/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0991 - val_loss: 0.1116\n",
      "Epoch 143/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0974 - val_loss: 0.1098\n",
      "Epoch 144/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0958 - val_loss: 0.1081\n",
      "Epoch 145/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0942 - val_loss: 0.1063\n",
      "Epoch 146/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0925 - val_loss: 0.1046\n",
      "Epoch 147/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0909 - val_loss: 0.1029\n",
      "Epoch 148/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0894 - val_loss: 0.1013\n",
      "Epoch 149/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0878 - val_loss: 0.0997\n",
      "Epoch 150/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 0.0980\n",
      "Epoch 151/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0848 - val_loss: 0.0964\n",
      "Epoch 152/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 0.0949\n",
      "Epoch 153/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0819 - val_loss: 0.0934\n",
      "Epoch 154/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 0.0918\n",
      "Epoch 155/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0791 - val_loss: 0.0903\n",
      "Epoch 156/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0777 - val_loss: 0.0889\n",
      "Epoch 157/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0764 - val_loss: 0.0874\n",
      "Epoch 158/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0751 - val_loss: 0.0860\n",
      "Epoch 159/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0738 - val_loss: 0.0846\n",
      "Epoch 160/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0725 - val_loss: 0.0832\n",
      "Epoch 161/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0712 - val_loss: 0.0819\n",
      "Epoch 162/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0700 - val_loss: 0.0805\n",
      "Epoch 163/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0688 - val_loss: 0.0793\n",
      "Epoch 164/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0676 - val_loss: 0.0780\n",
      "Epoch 165/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0665 - val_loss: 0.0767\n",
      "Epoch 166/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0653 - val_loss: 0.0755\n",
      "Epoch 167/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0642 - val_loss: 0.0743\n",
      "Epoch 168/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0631 - val_loss: 0.0731\n",
      "Epoch 169/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 0.0719\n",
      "Epoch 170/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0610 - val_loss: 0.0708\n",
      "Epoch 171/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0600 - val_loss: 0.0696\n",
      "Epoch 172/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0590 - val_loss: 0.0685\n",
      "Epoch 173/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0580 - val_loss: 0.0675\n",
      "Epoch 174/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0570 - val_loss: 0.0664\n",
      "Epoch 175/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0560 - val_loss: 0.0654\n",
      "Epoch 176/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0551 - val_loss: 0.0643\n",
      "Epoch 177/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0542 - val_loss: 0.0633\n",
      "Epoch 178/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0533 - val_loss: 0.0623\n",
      "Epoch 179/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0525 - val_loss: 0.0614\n",
      "Epoch 180/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0516 - val_loss: 0.0604\n",
      "Epoch 181/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0508 - val_loss: 0.0595\n",
      "Epoch 182/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0500 - val_loss: 0.0586\n",
      "Epoch 183/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0492 - val_loss: 0.0577\n",
      "Epoch 184/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0484 - val_loss: 0.0568\n",
      "Epoch 185/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0476 - val_loss: 0.0560\n",
      "Epoch 186/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0469 - val_loss: 0.0552\n",
      "Epoch 187/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0461 - val_loss: 0.0543\n",
      "Epoch 188/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0454 - val_loss: 0.0535\n",
      "Epoch 189/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0447 - val_loss: 0.0528\n",
      "Epoch 190/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0440 - val_loss: 0.0520\n",
      "Epoch 191/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0434 - val_loss: 0.0512\n",
      "Epoch 192/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0427 - val_loss: 0.0505\n",
      "Epoch 193/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0421 - val_loss: 0.0498\n",
      "Epoch 194/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0415 - val_loss: 0.0491\n",
      "Epoch 195/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0409 - val_loss: 0.0484\n",
      "Epoch 196/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0403 - val_loss: 0.0477\n",
      "Epoch 197/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0397 - val_loss: 0.0471\n",
      "Epoch 198/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0391 - val_loss: 0.0464\n",
      "Epoch 199/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0386 - val_loss: 0.0458\n",
      "Epoch 200/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0380 - val_loss: 0.0452\n",
      "Epoch 201/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0375 - val_loss: 0.0446\n",
      "Epoch 202/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0370 - val_loss: 0.0440\n",
      "Epoch 203/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0365 - val_loss: 0.0434\n",
      "Epoch 204/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0360 - val_loss: 0.0429\n",
      "Epoch 205/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0355 - val_loss: 0.0423\n",
      "Epoch 206/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0350 - val_loss: 0.0418\n",
      "Epoch 207/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0346 - val_loss: 0.0413\n",
      "Epoch 208/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0341 - val_loss: 0.0408\n",
      "Epoch 209/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0337 - val_loss: 0.0402\n",
      "Epoch 210/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0333 - val_loss: 0.0398\n",
      "Epoch 211/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0329 - val_loss: 0.0393\n",
      "Epoch 212/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0325 - val_loss: 0.0388\n",
      "Epoch 213/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0321 - val_loss: 0.0384\n",
      "Epoch 214/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0317 - val_loss: 0.0379\n",
      "Epoch 215/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0313 - val_loss: 0.0375\n",
      "Epoch 216/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0310 - val_loss: 0.0371\n",
      "Epoch 217/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0306 - val_loss: 0.0367\n",
      "Epoch 218/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0303 - val_loss: 0.0363\n",
      "Epoch 219/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0299 - val_loss: 0.0359\n",
      "Epoch 220/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0296 - val_loss: 0.0355\n",
      "Epoch 221/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0293 - val_loss: 0.0351\n",
      "Epoch 222/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0289 - val_loss: 0.0348\n",
      "Epoch 223/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0286 - val_loss: 0.0344\n",
      "Epoch 224/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0283 - val_loss: 0.0341\n",
      "Epoch 225/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0280 - val_loss: 0.0337\n",
      "Epoch 226/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0278 - val_loss: 0.0334\n",
      "Epoch 227/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0275 - val_loss: 0.0330\n",
      "Epoch 228/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0272 - val_loss: 0.0327\n",
      "Epoch 229/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0269 - val_loss: 0.0324\n",
      "Epoch 230/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0267 - val_loss: 0.0321\n",
      "Epoch 231/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0264 - val_loss: 0.0318\n",
      "Epoch 232/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0262 - val_loss: 0.0316\n",
      "Epoch 233/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0259 - val_loss: 0.0312\n",
      "Epoch 234/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0257 - val_loss: 0.0310\n",
      "Epoch 235/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0255 - val_loss: 0.0307\n",
      "Epoch 236/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0252 - val_loss: 0.0305\n",
      "Epoch 237/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0250 - val_loss: 0.0302\n",
      "Epoch 238/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0248 - val_loss: 0.0299\n",
      "Epoch 239/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0246 - val_loss: 0.0297\n",
      "Epoch 240/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0244 - val_loss: 0.0294\n",
      "Epoch 241/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0242 - val_loss: 0.0292\n",
      "Epoch 242/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0240 - val_loss: 0.0290\n",
      "Epoch 243/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0238 - val_loss: 0.0288\n",
      "Epoch 244/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0236 - val_loss: 0.0285\n",
      "Epoch 245/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0234 - val_loss: 0.0283\n",
      "Epoch 246/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0232 - val_loss: 0.0281\n",
      "Epoch 247/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0231 - val_loss: 0.0279\n",
      "Epoch 248/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0229 - val_loss: 0.0277\n",
      "Epoch 249/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0227 - val_loss: 0.0275\n",
      "Epoch 250/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0226 - val_loss: 0.0273\n",
      "Epoch 251/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0224 - val_loss: 0.0271\n",
      "Epoch 252/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0222 - val_loss: 0.0270\n",
      "Epoch 253/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0221 - val_loss: 0.0268\n",
      "Epoch 254/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0219 - val_loss: 0.0266\n",
      "Epoch 255/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0218 - val_loss: 0.0264\n",
      "Epoch 256/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0216 - val_loss: 0.0262\n",
      "Epoch 257/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0215 - val_loss: 0.0261\n",
      "Epoch 258/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0214 - val_loss: 0.0259\n",
      "Epoch 259/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0212 - val_loss: 0.0258\n",
      "Epoch 260/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0211 - val_loss: 0.0256\n",
      "Epoch 261/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0210 - val_loss: 0.0254\n",
      "Epoch 262/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0208 - val_loss: 0.0253\n",
      "Epoch 263/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0207 - val_loss: 0.0252\n",
      "Epoch 264/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0206 - val_loss: 0.0250\n",
      "Epoch 265/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0205 - val_loss: 0.0249\n",
      "Epoch 266/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0203 - val_loss: 0.0248\n",
      "Epoch 267/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0202 - val_loss: 0.0246\n",
      "Epoch 268/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0201 - val_loss: 0.0245\n",
      "Epoch 269/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0200 - val_loss: 0.0244\n",
      "Epoch 270/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0199 - val_loss: 0.0243\n",
      "Epoch 271/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0198 - val_loss: 0.0241\n",
      "Epoch 272/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0197 - val_loss: 0.0240\n",
      "Epoch 273/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0196 - val_loss: 0.0239\n",
      "Epoch 274/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0195 - val_loss: 0.0238\n",
      "Epoch 275/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0194 - val_loss: 0.0237\n",
      "Epoch 276/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0193 - val_loss: 0.0236\n",
      "Epoch 277/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0192 - val_loss: 0.0235\n",
      "Epoch 278/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0191 - val_loss: 0.0233\n",
      "Epoch 279/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0190 - val_loss: 0.0232\n",
      "Epoch 280/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0189 - val_loss: 0.0231\n",
      "Epoch 281/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0230\n",
      "Epoch 282/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0188 - val_loss: 0.0229\n",
      "Epoch 283/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0187 - val_loss: 0.0228\n",
      "Epoch 284/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0186 - val_loss: 0.0227\n",
      "Epoch 285/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0185 - val_loss: 0.0227\n",
      "Epoch 286/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0226\n",
      "Epoch 287/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0184 - val_loss: 0.0225\n",
      "Epoch 288/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0183 - val_loss: 0.0224\n",
      "Epoch 289/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0223\n",
      "Epoch 290/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0182 - val_loss: 0.0223\n",
      "Epoch 291/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0181 - val_loss: 0.0222\n",
      "Epoch 292/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0180 - val_loss: 0.0221\n",
      "Epoch 293/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0220\n",
      "Epoch 294/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0179 - val_loss: 0.0219\n",
      "Epoch 295/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0178 - val_loss: 0.0219\n",
      "Epoch 296/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0178 - val_loss: 0.0218\n",
      "Epoch 297/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0177 - val_loss: 0.0217\n",
      "Epoch 298/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0176 - val_loss: 0.0217\n",
      "Epoch 299/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0176 - val_loss: 0.0216\n",
      "Epoch 300/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0175 - val_loss: 0.0215\n",
      "Epoch 301/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0175 - val_loss: 0.0214\n",
      "Epoch 302/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0174 - val_loss: 0.0214\n",
      "Epoch 303/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 0.0213\n",
      "Epoch 304/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0173 - val_loss: 0.0213\n",
      "Epoch 305/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0172 - val_loss: 0.0212\n",
      "Epoch 306/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0172 - val_loss: 0.0211\n",
      "Epoch 307/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 0.0211\n",
      "Epoch 308/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0171 - val_loss: 0.0211\n",
      "Epoch 309/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0210\n",
      "Epoch 310/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0170 - val_loss: 0.0209\n",
      "Epoch 311/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0209\n",
      "Epoch 312/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0208\n",
      "Epoch 313/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0169 - val_loss: 0.0208\n",
      "Epoch 314/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0207\n",
      "Epoch 315/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0168 - val_loss: 0.0207\n",
      "Epoch 316/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0167 - val_loss: 0.0206\n",
      "Epoch 317/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0167 - val_loss: 0.0205\n",
      "Epoch 318/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0205\n",
      "Epoch 319/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0205\n",
      "Epoch 320/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0166 - val_loss: 0.0204\n",
      "Epoch 321/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0204\n",
      "Epoch 322/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0165 - val_loss: 0.0203\n",
      "Epoch 323/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0203\n",
      "Epoch 324/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0203\n",
      "Epoch 325/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0164 - val_loss: 0.0202\n",
      "Epoch 326/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0201\n",
      "Epoch 327/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0163 - val_loss: 0.0201\n",
      "Epoch 328/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0201\n",
      "Epoch 329/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0200\n",
      "Epoch 330/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0162 - val_loss: 0.0200\n",
      "Epoch 331/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0199\n",
      "Epoch 332/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0200\n",
      "Epoch 333/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0161 - val_loss: 0.0199\n",
      "Epoch 334/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0198\n",
      "Epoch 335/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0198\n",
      "Epoch 336/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0160 - val_loss: 0.0198\n",
      "Epoch 337/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0197\n",
      "Epoch 338/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0197\n",
      "Epoch 339/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0159 - val_loss: 0.0196\n",
      "Epoch 340/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0196\n",
      "Epoch 341/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0196\n",
      "Epoch 342/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0195\n",
      "Epoch 343/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0158 - val_loss: 0.0195\n",
      "Epoch 344/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0195\n",
      "Epoch 345/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0194\n",
      "Epoch 346/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0157 - val_loss: 0.0194\n",
      "Epoch 347/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0194\n",
      "Epoch 348/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0194\n",
      "Epoch 349/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0193\n",
      "Epoch 350/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0156 - val_loss: 0.0193\n",
      "Epoch 351/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0193\n",
      "Epoch 352/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0192\n",
      "Epoch 353/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0155 - val_loss: 0.0192\n",
      "Epoch 354/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0192\n",
      "Epoch 355/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0191\n",
      "Epoch 356/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0191\n",
      "Epoch 357/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0154 - val_loss: 0.0191\n",
      "Epoch 358/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0190\n",
      "Epoch 359/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0190\n",
      "Epoch 360/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0190\n",
      "Epoch 361/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0153 - val_loss: 0.0190\n",
      "Epoch 362/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0189\n",
      "Epoch 363/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0189\n",
      "Epoch 364/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0189\n",
      "Epoch 365/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0152 - val_loss: 0.0189\n",
      "Epoch 366/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0188\n",
      "Epoch 367/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0188\n",
      "Epoch 368/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0188\n",
      "Epoch 369/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0151 - val_loss: 0.0188\n",
      "Epoch 370/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0187\n",
      "Epoch 371/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0188\n",
      "Epoch 372/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0187\n",
      "Epoch 373/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0187\n",
      "Epoch 374/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0150 - val_loss: 0.0186\n",
      "Epoch 375/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0186\n",
      "Epoch 376/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0186\n",
      "Epoch 377/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0186\n",
      "Epoch 378/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0149 - val_loss: 0.0186\n",
      "Epoch 379/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0185\n",
      "Epoch 380/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0185\n",
      "Epoch 381/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0185\n",
      "Epoch 382/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0185\n",
      "Epoch 383/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0148 - val_loss: 0.0185\n",
      "Epoch 384/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0184\n",
      "Epoch 385/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0184\n",
      "Epoch 386/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0184\n",
      "Epoch 387/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0183\n",
      "Epoch 388/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0147 - val_loss: 0.0183\n",
      "Epoch 389/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0183\n",
      "Epoch 390/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0183\n",
      "Epoch 391/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0183\n",
      "Epoch 392/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0183\n",
      "Epoch 393/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0146 - val_loss: 0.0182\n",
      "Epoch 394/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0182\n",
      "Epoch 395/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0182\n",
      "Epoch 396/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0182\n",
      "Epoch 397/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0181\n",
      "Epoch 398/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0181\n",
      "Epoch 399/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0145 - val_loss: 0.0181\n",
      "Epoch 400/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0181\n",
      "Epoch 401/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0180\n",
      "Epoch 402/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0181\n",
      "Epoch 403/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0180\n",
      "Epoch 404/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0180\n",
      "Epoch 405/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0144 - val_loss: 0.0180\n",
      "Epoch 406/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0180\n",
      "Epoch 407/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0180\n",
      "Epoch 408/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0179\n",
      "Epoch 409/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0180\n",
      "Epoch 410/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0179\n",
      "Epoch 411/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0143 - val_loss: 0.0179\n",
      "Epoch 412/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0179\n",
      "Epoch 413/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0178\n",
      "Epoch 414/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0178\n",
      "Epoch 415/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0178\n",
      "Epoch 416/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0178\n",
      "Epoch 417/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0142 - val_loss: 0.0178\n",
      "Epoch 418/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0178\n",
      "Epoch 419/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0178\n",
      "Epoch 420/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0177\n",
      "Epoch 421/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0177\n",
      "Epoch 422/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0177\n",
      "Epoch 423/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0177\n",
      "Epoch 424/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0141 - val_loss: 0.0177\n",
      "Epoch 425/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 426/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0177\n",
      "Epoch 427/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 428/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 429/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 430/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 431/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0140 - val_loss: 0.0176\n",
      "Epoch 432/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0176\n",
      "Epoch 433/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 434/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 435/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 436/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 437/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0174\n",
      "Epoch 438/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 439/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0139 - val_loss: 0.0175\n",
      "Epoch 440/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0175\n",
      "Epoch 441/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 442/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 443/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 444/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 445/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 446/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0174\n",
      "Epoch 447/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0173\n",
      "Epoch 448/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0138 - val_loss: 0.0173\n",
      "Epoch 449/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 450/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 451/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 452/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 453/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 454/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0172\n",
      "Epoch 455/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 456/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0172\n",
      "Epoch 457/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0137 - val_loss: 0.0173\n",
      "Epoch 458/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 459/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 460/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 461/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0171\n",
      "Epoch 462/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 463/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 464/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0172\n",
      "Epoch 465/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0171\n",
      "Epoch 466/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0136 - val_loss: 0.0171\n",
      "Epoch 467/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 468/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 469/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 470/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 471/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 472/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 473/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0170\n",
      "Epoch 474/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 475/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 476/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0171\n",
      "Epoch 477/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0135 - val_loss: 0.0170\n",
      "Epoch 478/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 479/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 480/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 481/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 482/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 483/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 484/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 485/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0169\n",
      "Epoch 486/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0170\n",
      "Epoch 487/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0169\n",
      "Epoch 488/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0169\n",
      "Epoch 489/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0134 - val_loss: 0.0169\n",
      "Epoch 490/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 491/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 492/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 493/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 494/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 495/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 496/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 497/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 498/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 499/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 500/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0169\n",
      "Epoch 501/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 502/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0133 - val_loss: 0.0168\n",
      "Epoch 503/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 504/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 505/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 506/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 507/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 508/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 509/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0168\n",
      "Epoch 510/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 511/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 512/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 513/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 514/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 515/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 516/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0132 - val_loss: 0.0167\n",
      "Epoch 517/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 518/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 519/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 520/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 521/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 522/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 523/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 524/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 525/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0167\n",
      "Epoch 526/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 527/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 528/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 529/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 530/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 531/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 532/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0131 - val_loss: 0.0166\n",
      "Epoch 533/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 534/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 535/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 536/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 537/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 538/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 539/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 540/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 541/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 542/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0166\n",
      "Epoch 543/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 544/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 545/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 546/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 547/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 548/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 549/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 550/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0130 - val_loss: 0.0165\n",
      "Epoch 551/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 552/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 553/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 554/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 555/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 556/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 557/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 558/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 559/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 560/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0165\n",
      "Epoch 561/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 562/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 563/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 564/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 565/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 566/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 567/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 568/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 569/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 570/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0129 - val_loss: 0.0164\n",
      "Epoch 571/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 572/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 573/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 574/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 575/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 576/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 577/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 578/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 579/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0164\n",
      "Epoch 580/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 581/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 582/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 583/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 584/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 585/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 586/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 587/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 588/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 589/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 590/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 591/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 592/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 593/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0128 - val_loss: 0.0163\n",
      "Epoch 594/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 595/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 596/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 597/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 598/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 599/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 600/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 601/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0163\n",
      "Epoch 602/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 603/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 604/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 605/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 606/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 607/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 608/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 609/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 610/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 611/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 612/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 613/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 614/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 615/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 616/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 617/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0162\n",
      "Epoch 618/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0127 - val_loss: 0.0161\n",
      "Epoch 619/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 620/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 621/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 622/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 623/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 624/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 625/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 626/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 627/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 628/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 629/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 630/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 631/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0162\n",
      "Epoch 632/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 633/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 634/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 635/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 636/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 637/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 638/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 639/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 640/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 641/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 642/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 643/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 644/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0160\n",
      "Epoch 645/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0126 - val_loss: 0.0161\n",
      "Epoch 646/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 647/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 648/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 649/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 650/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 651/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 652/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 653/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 654/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 655/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0161\n",
      "Epoch 656/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 657/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 658/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 659/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 660/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 661/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 662/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 663/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 664/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 665/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 666/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 667/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 668/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 669/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 670/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 671/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 672/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 673/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0159\n",
      "Epoch 674/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0125 - val_loss: 0.0160\n",
      "Epoch 675/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 676/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 677/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 678/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 679/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 680/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 681/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 682/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 683/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 684/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0160\n",
      "Epoch 685/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 686/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 687/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 688/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 689/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 690/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 691/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 692/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 693/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 694/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 695/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 696/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 697/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 698/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 699/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 700/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 701/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 702/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 703/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 704/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 705/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0158\n",
      "Epoch 706/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0124 - val_loss: 0.0159\n",
      "Epoch 707/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0159\n",
      "Epoch 708/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 709/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 710/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 711/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 712/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0159\n",
      "Epoch 713/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 714/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0159\n",
      "Epoch 715/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 716/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 717/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 718/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 719/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 720/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 721/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 722/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 723/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 724/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 725/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 726/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 727/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 728/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 729/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 730/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 731/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 732/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 733/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 734/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 735/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 736/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 737/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 738/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0158\n",
      "Epoch 739/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0123 - val_loss: 0.0157\n",
      "Epoch 740/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 741/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 742/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 743/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 744/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 745/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 746/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 747/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 748/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 749/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 750/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 751/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 752/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 753/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 754/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 755/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 756/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 757/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 758/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 759/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 760/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 761/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 762/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 763/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 764/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 765/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 766/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 767/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 768/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 769/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 770/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 771/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 772/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 773/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0157\n",
      "Epoch 774/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0122 - val_loss: 0.0156\n",
      "Epoch 775/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 776/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 777/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 778/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 779/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 780/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 781/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0157\n",
      "Epoch 782/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 783/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 784/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 785/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 786/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 787/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 788/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 789/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 790/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 791/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 792/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 793/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 794/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 795/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 796/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 797/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 798/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 799/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 800/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 801/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 802/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 803/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 804/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0156\n",
      "Epoch 805/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 806/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 807/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 808/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 809/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 810/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 811/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 812/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0121 - val_loss: 0.0155\n",
      "Epoch 813/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 814/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 815/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 816/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 817/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 818/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 819/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 820/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 821/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 822/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 823/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 824/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 825/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 826/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 827/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 828/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 829/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 830/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 831/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 832/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 833/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 834/1000\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 835/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 836/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 837/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 838/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 839/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0155\n",
      "Epoch 840/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 841/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 842/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 843/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 844/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 845/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 846/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 847/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 848/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 849/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 850/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 851/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0120 - val_loss: 0.0154\n",
      "Epoch 852/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 853/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 854/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 855/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 856/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 857/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0155\n",
      "Epoch 858/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 859/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 860/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 861/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 862/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 863/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 864/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 865/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 866/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 867/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 868/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 869/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 870/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 871/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 872/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 873/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 874/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 875/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 876/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 877/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 878/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 879/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 880/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 881/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 882/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 883/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 884/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 885/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 886/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 887/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 888/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0154\n",
      "Epoch 889/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 890/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 891/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0119 - val_loss: 0.0153\n",
      "Epoch 892/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 893/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 894/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 895/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 896/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 897/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 898/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 899/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 900/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 901/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 902/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 903/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 904/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 905/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 906/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 907/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 908/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 909/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 910/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 911/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 912/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 913/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 914/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 915/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 916/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 917/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 918/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 919/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 920/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 921/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 922/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 923/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 924/1000\n",
      "36/36 [==============================] - ETA: 0s - loss: 0.011 - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 925/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 926/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 927/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0153\n",
      "Epoch 928/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 929/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 930/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 931/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 932/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 933/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 934/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0118 - val_loss: 0.0152\n",
      "Epoch 935/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 936/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 937/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 938/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 939/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 940/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 941/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 942/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 943/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 944/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 945/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 946/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 947/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 948/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 949/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 950/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 951/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 952/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 953/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 954/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 955/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 956/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 957/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 958/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 959/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 960/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 961/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 962/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 963/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 964/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 965/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 966/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 967/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 968/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 969/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 970/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 971/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 972/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 973/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0152\n",
      "Epoch 974/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 975/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 976/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 977/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 978/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 979/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0150\n",
      "Epoch 980/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0117 - val_loss: 0.0151\n",
      "Epoch 981/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 982/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 983/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 984/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 985/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 986/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 987/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 988/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 989/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 990/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 991/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 992/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 993/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 994/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 995/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 996/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 997/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n",
      "Epoch 998/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 999/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0150\n",
      "Epoch 1000/1000\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.0116 - val_loss: 0.0151\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=16, activation='tanh', input_shape=(X_train.shape[-1],)))\n",
    "model.add(Dense(units=8, activation='tanh'))\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(optimizer=Adam(lr=0.00001), loss='mse')\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=1000, epochs=1000, shuffle=True, verbose=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "64dddcbcaec4bb6385c138499a04ba94b2e3f1c047b570a9b11b84db929d6a00"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
